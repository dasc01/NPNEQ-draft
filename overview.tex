\section{Overview And Executive Summary Of Accomplishment}
\label{sec:overview}

The development of quantum mechanics in the early 20th century lead to a paradigm shift in materials science and engineering that resulted in, for example, the invention of semiconductor devices. The emergence of Density Functional Theory (DFT) together with breakthroughs in algorithms \cite{Cooley1965,Martin1988} and rapid increases in computer performance contributed significantly to the success of modern electronic structure theory. In addition, cross-validation of theory with state-of-art experiments, such as angle resolved photo emission spectroscopy, played a key role in advancing the use of DFT, as it was found to have an optimal balance of accuracy and computational expense. This progresses in the 20th century was, however, mostly limited to describing time independent quantum mechanical problems. Towards the end of century, time dependent versions of DFT (TDDFT) began to emerge \cite{RungeGross1984} and together with the progresses in ultrafast experimental techniques and continued advancements in high performance computing (HPC), we are now positioned to probe into phenomena that require the use of the time dependent version of the Schr\"{o}dinger equation, where spin, electronic, and ionic degree of freedoms evolve in a coupled manner. 

It is in this context that the DOE CMS Software Center for Nonperturbative Studies of Functional Materials under Nonequilibrium Conditions (NPNEQ) was launched in September 2019 with the initial goal of developing and distributing an open source real-time TDDFT (RT-TDDFT) code {\it optimized for the current and future DOE Leadership Class HPC systems} such as Sierra (LLNL: 125 petaflops, active from 2018), El Capitan (LLNL: 1.5 exaflops, 2023), Perlmutter (NERSC: 64 petaflops, phase I active from June 2021), Summit (200 petaflops, active from 2018), Frontier (ORNL, 1.5 exaflops, 2023), Aurora (ANL, 1 exaflops, 2021). 

As of June 2021, the RT-TDDFT software named INQ has been successfully implemented and released with all of the functions necessary for calculating ground state electronic structure as well as for time propagation. Rigorous tests for scientific research such as reproducibility of relevant physical quantities have been performed based on comparisons with well established DFT (QE/Qbox) and TDDFT (Octopus/Qb@ll) codes. Optimization on GPU based architectures has been successfully carried out together with an MPI parallel implementation, where software design ({\bf Correa}), development ({\bf Andrade}) and testing ({\bf Pemmaraju}) efforts were closely coordinated by the PI {\bf Ogitsu} through extensive online meetings (two two-hours meeting per week). \emph{ We have successfully executed our software development plan: INQ has achieved excellent parallel performance using multiple GPUs with standard DFT (GGA) based on the band parallelization scheme. A comprehensive report describing the INQ code and its future extension plans has been summarized and submitted to a peer reviewed journal.\cite{andrade2021inq}}

Currently existing GPU ready (TD)DFT codes have been focused on delivering GPU parallel performance for specific types of simulations, such as hybrid exchange-correlation (BigDFT)\cite{BigDFT2018},  GW calculations (VASP) \cite{vasp2012,vasp2012b,vasp2018,vasp2019}, and CCSD (NWChem).\cite{NWChem2013} These types of calculations intrinsically have a high arithmetic intensity, or k-point parallelization \cite{QE2017,QE2020} and fewer communication requirements compared to band or grid parallelization strategies. We note that (TD)DFT simulations of realistic materials and systems often require large system sizes in order to account for factors such as defects, impurities and interfaces. In order to address the length-scales required for these types of calculations, band and grid parallel schemes become more important than parallelization over k-points in minimizing the time to solution. At the time of writing this report, only the real space GGA (TD)DFT codes\cite{andrade2012time,andrade2013real,SparcX2021} have been able to take full advantage of the computing power of parallel GPU type architectures, which suggests that a lack of sufficient interconnect bandwidth in the current HPC platforms may pose a significant challenge to codes that are based on parallel 3D FFT routines, as 3D FFT are known to require high memory and interconnect bandwidth.\cite{heFFTe2020}

Our software development strategy is comprehensive and aims to facilitate advancement of theory by highly programmable design. As such, scientists who are not expert programmers are able to use our software to implement newly developed theories and algorithms with ease, thereby helping to expand the user community and extend the functionality of INQ to address the broad scientific community. Through extensive communications with both scientific and HPC communities, it was suggested that the basic design of the code needs to have significant flexibility in order to accommodate the rapidly evolving HPC system architectures as well as the development of the RT-TDDFT theory itself. 

An important HPC example is intimately related to the critical issue of non uniform memory access (NUMA) and quickly evolving its solution. The current challenge in parallel GPU optimization is that the code design needs to take redundant memory architectures with extremely uneven computational power (GPU$>>$CPU), which in turn requires redundant interconnect architecture, explicitly into consideration. In short, all of the data needs to be on the fast GPU in order to exploit the power of GPU and avoid additional CPU-GPU communication overhead. We note that the cost of interconnect represents a significant portion of the total cost of an HPC system, therefore, dual interconnect architectures are not ideal from the viewpoint of cost per performance. 

In considering our future plans, we recognize that this issue is well recognized by the HPC community and industry, and the solutions are coming. For example, the Apple M1 chip based on the ARM architecture takes a system on chip approach where a set of memory together with CPU and GPU are integrated on one chip. An HPC system based on such an unique memory architecture could eliminate the redundant interconnect as is seen in the current world fastest supercomputer, Fugaku. The A64FX chip used in Fugaku is based on the ARM architecture with an extension called scalable vector extension (SVE), whose modified version is now implemented in the ARMv9 instruction set as SVE2. NVIDIA is currently in the process of acquiring an ARM company possibly suggesting a significant architecture change in HPC systems in the next several years.

 The uncertainty in future HPC system architectures combined with the expected growth in a broader range of non-specialist researchers who would like to use our software led to the following design choices. While our software development platform github/gitlab offers elaborate functionality for developing and testing software, the workflow is specific to the software so that it has to be drafted by the software developers. Accordingly, we are developing a workflow specific to the INQ code including automated tests performed over multiple HPC systems, which is described in a document accessible through the repository. In addition, the users/programmers guides of INQ software will be published, which will take feedback from our user base and our Scientific Advisory Board (SAB) into consideration. This input will be critical for an open source software development project like ours, where we intend to have the code shared among all relevant stake holders such as researchers in scientific and HPC communities, and from industry.
 
 While the software development and validation tasks were being executed, the rest of team members tasked for experiments ({\bf Lindenberg}) and interpretation ({\bf Tan} and {\bf Prendergast}) prepared for the collaborative research projects for the second half of our funding period that take full advantage of the INQ code and ultrafast experimental capabilities. In particular, our combined experimental and computational team will focus on code validation through scientific research that involves phenomena intimately related to coupled quantum dynamics in the nonperturbative limit. Coordination in these efforts, including the INQ software development, were performed through communication during all-hands meetings that were held roughly every 6 weeks and a separate monthly meeting focusing on specific technical issues.
 
 Details on the research and further software development plans, the results of our efforts during the first two years including the one that made a crucial contribution to our future research plans, the tight-binding model for coupled spin-electron-ion quantum dynamics developed by our PD, {\bf Rajuprohit}, under the supervision of {\bf Tan}, software dissemination and outreach plans are described in the next section.
 

\clearpage
