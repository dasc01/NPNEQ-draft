\section{Overview And Executive Summary Of Accomplishment}
\label{sec:overview}
{\small
\begin{itemize}
    \item \textcolor{red}{2 pages max.}
    \begin{enumerate}
        \item \textcolor{red}{Provide a concise introduction to the organization and research being pursued.}
        \item \textcolor{red}{Briefly sketch the background leading to the 2019 proposal, the gaps in scientific knowledge that the CMS research aims to fill, and the vision and strategic objectives of the project.}
        \item \textcolor{red}{Include an Executive Summary of the progress made since September 2019 toward
meeting its strategic objectives, making clear how the accomplishments described in detail in Section 6 below fit together as part of a synergistic research program.}
    \end{enumerate}
    \item \textcolor{red}{Review criteria for Scientific and/or Technical Merit of the Proposed Research:}
    \begin{itemize}
        \item \textcolor{red}{What new capability/functionality will the proposed software provide to the materials research community? How will the research plan attain the 4-year research and software/data goals?}
        \item \textcolor{red}{Comment on the novelty and scientific value of the proposed research. }
        \item \textcolor{red}{How widespread would be the interest in the software within the materials research community?}
        \item \textcolor{red}{How does the proposed work compare with other efforts in its field, both in terms of scientific and/or technical merit and originality?}
        \item \textcolor{red}{Comment on the progress and impact for the current award period.}
    \end{itemize}

\end{itemize}
}

\clearpage

The development of quantum mechanics in early 20th century lead to paradigm shift in materials science and engineering that were witnessed as, for example, invention of semiconductor devices etc. 
Emergence of Density Functional Theory (DFT) \cite{HohenbergKohn1964,KohnSham1965} together with breakthrough in algorithmic developments \cite{cooley1965,Cohen1975,Hamann1979,CarParrinello1985,Martin1988,Hamann1989,Vanderbilt1990,Payne1992,Bloechl1994,Klesse1999} and fast growing performance of computers contributed significantly to advancement of the modern electronic structure theory where the state-of-art experiments of the day such as angle resolved photo emission and parameter free DFT band structure calculations helped cross-validating theory and experiments. 
Note that DFT's great practicality, reasonable accuracy combined with affordable computational cost, was an important factor though not often credited.
Those excellent progresses in the 20th century were, however, mostly limited to properties described as the time independent quantum mechanical problems. 
Towards the end of century, time dependent version of DFT (TDDFT) emerged \cite{RungeGross1984} together with the progress in ultrafast experimental techniques as well as the dramatic advancement in high performance computing (HPC) driven by the architectural transition into massive parallel GPU computing.
Thanks to that, we are now positioned to probe into phenomena that are described by time dependent version of Schr\"{o}dinger equation, where spin, electronic, and ionic degree of freedoms evolve in a coupled manner.

It is in this context that DOE CMS Software Center for Nonperturbative Studies of Functional Materials under Nonequilibrium Conditions (NPNEQ) was launched in September 2019 with the initial goal of developing and distributing Open Source real-time TDDFT (RT-TDDFT) software \emph{optimized for the current and future DOE Leadership Class HPC systems} such as 
	Sierra (LLNL: 125 petaflops, active from 2018), 
	El Capitan (LLNL: 1.5 exaflops, 2023), 
	Perlmutter (NERSC: 64 petaflops, phase I active from June 2021), 
	Summit (200 petaflops, active from 2018), 
	Frontier (ORNL, 1.5 exaflops, 2023), 
	and Aurora (ANL, 1 exaflops, 2021). 

As of June 2021, the RT-TDDFT software named INQ, whose basic functions are necessary for calculating ground state electronic structure as well as its time propagation have been successfully implemented. 
Rigorous tests for scientific research such as reproducibility of relevant physical quantities have been being performed based on comparisons with well established DFT (Quantum Espresso/Qbox) and TDDFT (Octopus/Qb@ll) codes. 
GPU optimization has been successfully performed together with MPI parallel implementation, where software design ({\bf Correa}), development ({\bf Andrade}) and testing ({\bf Pemmaraju}) effort were closely coordinated with the PI {\bf Ogitsu}'s oversight, fully leveraging extensive online meetings (two 2-hour meetings per week). 
\emph{
	We have successfully executed our software development plan: INQ has now achieved excellent parallel performance using multiple GPUs with a standard DFT (GGA) based on band parallelization scheme. 
	The comprehensive report about the INQ code and its future extension plan has been summarized and submitted to a peer-reviewed journal. \cite{andrade2021inq}
}

Currently existing GPU-ready (TD)DFT codes are mostly focused on delivering GPU parallel performance only for specific types of simulations such as, hybrid exchange-correlation (BigDFT) \cite{BigDFT2018} and GW calculations (VASP),\cite{vasp2012,vasp2012b,vasp2018,vasp2019} CCSD (NWChem),\cite{NWChem2013} which have intrinsically high arithmetic intensity, or k-point parallelization (QE)\cite{QE2017,QE2020} that requires less MPI communication compared to band/grid parallelization. 
Note that (TD)DFT simulation of realistic system often requires large systems in order to take relevant factors, such as defects and interface, to material property (or device performance), and in this case, band and grid parallel become more important than k-point parallel in minimizing time to solution. 
At the time of writing this report, only real space (TD)DFT codes\cite{andrade2012time,andrade2013real,SparcX2021} in GGA level simulations are able to take advantage of the computing power of GPU suggesting that lack of interconnect bandwidth may be the root of above observation since parallel 3D-FFT is known to demand high memory and interconnect bandwidths.\cite{heFFTe2020}

Our software development strategy is comprehensive and aims to facilitate advancement of theory by highly programmable design so as a scientist who is not an expert programmer to be able to implement newly developed theory/algorithm into INQ at ease, which will help expanding the user community while extending the functionality of INQ available to scientific community. 
Through extensive communications with both scientific and HPC communities suggested that basic design of code needs to have significant flexibility in adopting code design to rapidly evolving the HPC system architecture and as the theory of RT-TDDFT. 

An important HPC example is intimately related to the critical issue of non uniform memory access (NUMA) and quickly developing a solution. 
The current challenge in parallel GPU optimization is that the code design needs to take redundant memory architecture with extremely uneven computational powers (\(\mathrm{GPU} >> \mathrm{CPU}\)), which in turn requires redundant interconnect architecture, explicitly into consideration.
In short, all the data need to be on the fast GPU in order to exploit the power of GPU and avoid additional CPU-GPU communication overhead.
We note that the cost of interconnect occupies significant portion of the total cost of a HPC system, therefore, the dual interconnect architecture is not ideal from the viewpoint of cost per performance.

In considering future plan, we must note that the issue is well recognized by the HPC community and industry, and the solutions are coming. 
For example, Apple M1 chip based on ARM architecture takes system on chip approach where a set of memory together with CPU and GPU are integrated on one chip. 
A HPC system based on such a unique memory architecture can eliminate the redundant interconnect as is seen in the current world fastest supercomputer, Fugaku. 
A64FX chip used in Fugaku is based on ARM architecture with an extension called scalable vector extension (SVE), whose modified version is now implemented in ARMv9 instruction set as SVE2. NVIDIA is currently in the process of acquiring ARM company strongly suggesting a significant change in the architecture of DOE Leadership Class HPC systems in the next several years.

The anticipated change of the HPC system architecture combined with expected growth in number of contributors who are not expert programmers made us the following choice. 
While our software development platform github/gitlab offers an elaborate functionality for developing and testing software, the workflow is specific to the software so that it has to be drafted by the software developers. 
Accordingly, we are developing a workflow specific to the INQ code including automated tests performed over multiple HPC systems, which is described in a document accessible through the repository. 
In addition, the guides for users and developers of INQ software will be published, which will take feedback from users and SABs into consideration. 
These will be critical factors for an open source software development project like ours whose value have to be shared among of all the stake holders such as researchers in scientific and HPC communities, and HPC industry.

While the software development and validation tasks were being executed, the rest of team members were tasked for experiments ({\bf Lindenberg}) and interpretation ({\bf Tan} and {\bf Prendergast}) prepared for the collaborative research projects for the second half of our funding period that take full advantage of INQ code and ultrafast experimental capability, which would play the role of ultimate validation through scientific researches that involves phenomena intimately related to coupled quantum dynamics in nonperturbative limit. 
Coordination in those efforts including INQ software development were performed through communication during all hands meetings held roughly every 6 weeks and a separate monthly meeting focusing more technical issues.

Details on the research and further software development plans, the results of our efforts during the first two years including the one that made a crucial contribution to our future research plans, the tight-binding model for coupled spin-electron-ion quantum dynamics developed by our PD, {\bf Rajuprohit}, under the supervision of {\bf Tan}, software dissemination and outreach plans are described in the next section.
 
\clearpage

\textcolor{red}{below should be in the next section perhaps}

Understanding the concept of computational intensity is crucial in designing HPC software and hardware. 
Software with a given algorithm will show a certain ratio of number of floating point operation per machine cycle/number of data copy between memory and register per machine cycle/number of data copy between (MPI) processes per machine cycle, though it may vary slightly depending on some indirect factors.
By comparing these ratios between a given simulation against that of hardware (floating point operation count, memory bandwidth, interconnect bandwidth), one can understand approximate upper limit of software performance. 
In order to understand the performance behavior of parallel GPU (TD)DFT software on a few target HPC systems, Ogitsu is currently in communication with Prof. Stanimire Tomov of U. Tennesee, whose team developed GPU ready parallel 3D-FFT library named heFFTe, which is optimized for Summit at ORNL. 
As it is known, computationally expensive parts of a GGA/LDA level DFT simulation based on planewave pseudopotential algorithm are orthonormalization of wavefunction, 3D-FFT, and non-local pseudopotential. 
Accordingly, the performance of 3D-FFT often becomes a good predictor of overall performance.

\clearpage

