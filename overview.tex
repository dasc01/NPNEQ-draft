\section{Overview and Executive Summary of Accomplishments}
\label{sec:overview}

The development of quantum mechanics in the early 20th century led to a paradigm shift in materials science and engineering that resulted in, for example, the invention of semiconductor devices. The emergence of Density Functional Theory (DFT) together with breakthroughs in algorithms \cite{Cooley1965,Martin1988} and rapid increases in computer performance contributed significantly to the success of modern electronic structure theory. In addition, cross-validation of theory with state-of-art experiments, such as angle resolved photo emission spectroscopy, played a key role in advancing the use of DFT, as it was found to have an optimal balance of accuracy and computational expense. However, progress in the 20th century was mostly limited to describing time-independent quantum mechanical problems. Towards the end of the century, time-dependent versions of DFT (TDDFT) began to emerge \cite{RungeGross1984} and together with the progress in ultrafast experimental techniques and continued advancements in high performance computing (HPC), we are now positioned to probe into phenomena that require the use of the time-dependent version of the Schr\"{o}dinger equation, where spin, electronic, and ionic degree of freedoms evolve in a coupled manner. 

In this context, the DOE CMS Software Center for Nonperturbative Studies of Functional Materials under Nonequilibrium Conditions (NPNEQ) was launched in September 2019 with the initial goal of developing and distributing an open source real-time TDDFT (RT-TDDFT) code {\it optimized for the current and future DOE Leadership Class HPC systems} such as Sierra (LLNL: 125 petaflops, active from 2018), El Capitan (LLNL: 1.5 exaflops, 2023), Perlmutter (NERSC: 64 petaflops, phase I active from June 2021), Summit (200 petaflops, active from 2018), Frontier (ORNL, 1.5 exaflops, 2023), Aurora (ANL, 1 exaflops, 2021). 

As of June 2021, the RT-TDDFT software named INQ has been successfully implemented and released with all of the functions necessary for calculating ground state electronic structure and time propagation. Rigorous tests for scientific research such as reproducibility of relevant physical quantities have been performed based on comparisons with well established DFT (QE/Qbox) and TDDFT (Octopus/Qb@ll) codes. Optimization on GPU-based architectures has been successfully carried out together with a MPI parallel implementation, where software design ({\bf Correa}), development ({\bf Andrade}) and testing ({\bf Pemmaraju}) efforts were closely coordinated by the PI {\bf Ogitsu} through extensive online meetings (two 2-hour meetings per week). \emph{ We have successfully executed our software development plan: INQ has achieved excellent parallel performance using multiple GPUs with standard DFT (GGA) based on the band parallelization scheme. See Figures~\ref{fig:scaling_strong} and \ref{fig:scaling_gpu_vs_cpu} in page~\pageref{fig:scaling_strong}. A comprehensive report describing the INQ code and its future extension plans has been summarized and submitted to a peer reviewed journal~\cite{andrade2021inq}.} 

Current GPU-ready (TD)DFT codes focus on delivering GPU parallel performance for specific types of simulations, such as hybrid exchange-correlation (BigDFT)\cite{BigDFT2018},  GW calculations (VASP)~\cite{vasp2012,vasp2012b,vasp2018,vasp2019}, and CCSD (NWChem)~\cite{NWChem2013}. These calculations intrinsically have a high arithmetic intensity, or k-point parallelization~\cite{QE2017,QE2020} and fewer communication requirements compared to band or grid parallelization strategies. (TD)DFT simulations of realistic materials and systems often require large system sizes in order to account for factors such as defects, impurities, and interfaces. To address the length-scales required for these types of calculations, band and grid parallel schemes are more important than parallelization over k-points in minimizing time to solution. At the time of this writing, only the real space GGA (TD)DFT codes~\cite{andrade2012time,andrade2013real,SparcX2021} have taken full advantage of the computing power of parallel GPU-type architectures, which suggests that a lack of sufficient interconnect bandwidth in the current HPC platforms may pose a significant challenge to codes that are based on parallel 3D FFT routines, as 3D FFT are known to require high memory and interconnect bandwidth~\cite{heFFTe2020}. See Appendix~\ref{sec:hpc} for more detail.

Our software development strategy is comprehensive and aims to facilitate advancement of theory by highly programmable design. As such, scientists who are not expert programmers can use our software to implement newly developed theories and algorithms with ease, which will expand the user community and extend the functionality of INQ. Through communications with the scientific and HPC communities, it was suggested that the basic code design must have significant flexibility to accommodate the rapidly evolving HPC system architectures as well as the development of the RT-TDDFT theory itself. 

An important HPC example is intimately related to the critical issue of non-uniform memory access (NUMA) and quickly evolving its solution. The current challenge in parallel GPU optimization is that the code design needs to take redundant memory architectures with extremely uneven computational power (GPU$>>$CPU), which in turn requires a redundant interconnect architecture, explicitly into consideration. In short, all of the data needs to be on the fast GPU in order to exploit the power of GPU and avoid additional CPU-GPU communication overhead. We note that the cost of interconnect represents a significant portion of the total cost of an HPC system; therefore, dual interconnect architectures are not ideal from the viewpoint of cost per performance. 

We recognize that this issue is well recognized by the HPC community and industry, and the solutions are coming. For example, the Apple M1 chip based on the ARM architecture takes a system on chip approach in which a set of memory together with CPU and GPU are integrated on one chip. An HPC system based on such a unique memory architecture could eliminate the redundant interconnect as is seen in the world's current fastest supercomputer, Fugaku. The A64FX chip used in Fugaku is based on the ARM architecture with an extension called scalable vector extension (SVE), whose modified version is now implemented in the ARMv9 instruction set as SVE2. NVIDIA is currently in the process of acquiring an ARM company, possibly suggesting a significant architecture change in HPC systems in the next several years.

 The uncertainty in future HPC system architectures combined with the expected growth in a broader range of non-specialist researchers who would like to use our software led to the following choices. While our software development platform github/gitlab offers elaborate functionality for developing and testing software, the workflow is specific to the software and must be drafted by the software developers. Accordingly, we are developing a workflow specific to the INQ code including automated tests performed over multiple HPC systems, which is described in a document accessible through the repository. Also, the users/programmers guides of INQ software will be published, which will consider feedback from our user base and our Scientific Advisory Board (SAB). This input will be critical for an open source software development project like ours, where we intend to share the code among all relevant stakeholders such as researchers in scientific and HPC communities, and from industry.
 
 While the software development and validation tasks were being executed, the rest of team members tasked for experiments ({\bf Lindenberg}) and interpretation ({\bf Tan} and {\bf Prendergast}) prepared for the collaborative research projects for the second half of our funding period that take full advantage of the INQ code and ultrafast experimental capabilities. In particular, our combined experimental and computational team will focus on code validation through scientific research that involves phenomena intimately related to coupled quantum dynamics in the nonperturbative limit. Coordination in these efforts, including the INQ software development, were performed through communication during all-hands meetings that were held roughly every 6 weeks and a separate monthly meeting focusing on specific technical issues.
 
 The next section details further research and software development plans, the results of our efforts during the first two years, including the one that made a crucial contribution to our future research plans, the tight-binding model for coupled spin-electron-ion quantum dynamics developed by our PD {\bf Rajuprohit} under the supervision of {\bf Tan}, software dissemination, and outreach plans.
 

\clearpage
