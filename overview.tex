\section{Overview And Executive Summary Of Accomplishment}
\label{sec:overview}
{\small
\begin{itemize}
    \item \textcolor{red}{2 pages max.}
    \begin{enumerate}
        \item \textcolor{red}{Provide a concise introduction to the organization and research being pursued.}
        \item \textcolor{red}{Briefly sketch the background leading to the 2019 proposal, the gaps in scientific knowledge that the CMS research aims to fill, and the vision and strategic objectives of the project.}
        \item \textcolor{red}{Include an Executive Summary of the progress made since September 2019 toward
meeting its strategic objectives, making clear how the accomplishments described in detail in Section 6 below fit together as part of a synergistic research program.}
    \end{enumerate}
    \item \textcolor{red}{Review criteria for Scientific and/or Technical Merit of the Proposed Research:}
    \begin{itemize}
        \item \textcolor{red}{What new capability/functionality will the proposed software provide to the materials research community? How will the research plan attain the 4-year research and software/data goals?}
        \item \textcolor{red}{Comment on the novelty and scientific value of the proposed research. }
        \item \textcolor{red}{How widespread would be the interest in the software within the materials research community?}
        \item \textcolor{red}{How does the proposed work compare with other efforts in its field, both in terms of scientific and/or technical merit and originality?}
        \item \textcolor{red}{Comment on the progress and impact for the current award period.}
    \end{itemize}

\end{itemize}
}

\clearpage

Development of quantum mechanics in early 20th century is responsible for development of materials science and engineering witnessed in, for example, invention of semiconductor devices etc. Development of Density Functional Theory (DFT)\cite{HohenbergKohn1964,KohnSham1965} together with breakthrough in algorithmic developments\cite{cooley1965,Cohen1975,Hamann1979,CarParrinello1985,Martin1988,Hamann1989,Vanderbilt1990,Payne1992,Bloechl1994,Klesse1999} and fast growing performance of computers contributed significantly to development of the modern electronic structure theory where the state-of-art experiments of the day such as angle resolved photo emission and parameter free DFT band structure calculation were directry compared each other, where DFT's great practicality, i.e., reasonable accuracy combined with affordable computational cost, was not often credited yet an important factor. Those excellent progresses in 20th century were, however, mostly limited to properties described within time independent quantum mechanics problems. Towards the end of century, time dependent version of DFT (TDDFT) emerged \cite{RungeGross1984} and together with the progresses in ultrafast experimental techniques, we are now positioned to probe into phenomena that are described by time dependent version of Schr\"{o}dinger equation, where spin, electronic, and ionic degree of freedoms evolve in a coupled manner. 

It is in this context, DOE CMS Software Center for Nonperturbative Studies of Functional Materials under Nonequilibrium Conditions (NPNEQ), was launched in September 2019 with the initial goal of develop and distribute Open Source real-time TDDFT (RT-TDDFT) software optimized for the current and future DOE Leadership Class HPC systems such as Sierra (LLNL: 125 petaflops, active from 2018), El Capitan (LLNL: 1.5 exaflops, 2023), Perlmutter (NERSC: 64 petaflops, phase I active from June 2021), Summit (200 petaflops, active from 2018), Frontier (ORNL, 1.5 exaflops, 2023), Aurora (ANL, 1 exaflops, 2021). 

As of June 2021, the RT-TDDFT software named INQ, whose basic functions necessary for calculating ground state electronic structure as well as its time propagation method have been successfully implemented. Rigorous tests for scientific research such as reproducibility of relevant physical quantities have been being performed based on comparisons with well established DFT (QE/Qbox) and TDDFT (Octopus/Qb@ll) codes. GPU optimization has been successfully performed together with MPI parallel implementation, where software design ({\bf Correa}), development ({\bf Andrade}) and testing ({\bf Pemmaraju}) effort were closely coordinated with the PI {\bf Ogitsu}'s oversight fully leveraging extensive online meetings (two two-hours meeting per week). {\it We have successfully executed our software development plan: INQ has now achieved excellent parallel performance using multiple GPUs with a standard DFT (GGA) based on band parallelization scheme. The comprehensive report about the INQ code and its future extension has been summarized and submitted to a peer reviewed journal.}

Currently existing GPU ready DFT/TDDFT codes are focused on delivering GPU parallel performance only for specific types of simulations such as, hybrid exchange-correlation and GW calculations (VASP), CCSD (NWChem), which have intrinsically high arithmetic intensity, or k-point parallelization (QE) that requires  fewer MPI communications compared to band/grid parallelization. Note that (TD)DFT simulation of realistic system often requires large system in order to take relevant factors, such as defects and interface, to material property (or device performance), and in this case, band and grid parallel become more important than k-point parallel in minimizing time to solution. {\color{red} mention OCTOPUS and SPARC-X, which are real space code}

Our software development strategy is comprehensive and aims to facilitate advancement of theory by highly programmable design so as a scientist who is not an expert programmer to be able to implement newly developed theory/algorithm into INQ at ease, which will help expanding the user community while extending the functionality of INQ available to scientific community. The lessens learned from the initial two years of our project through extensive communications with both scientific and HPC communities indicated that basic design of code needs to have significant flexibility in adopting code design to rapid evolution of the HPC systems as well as the theory of RT-TDDFT. 

An important HPC example is intimately related to the critical issue of non uniform memory access (NUMA) and quickly evolving its solution. The current challenge in parallel GPU optimization is that the code design needs to take redundant memory architecture with extremely uneven computational powers (GPU$>>$CPU), which in turn requires redundant interconnect architecture, explicitly into consideration. In short, all the data need to be on the fast GPU in order to exploit the power of GPU and avoid additional CPU-GPU communication overhead. While Summit/Sierra have direct GPU-GPU interconnect (NVIDIA GPUDirect RDMA) in addition to CPU-CPU interconnect, direct GPU-GPU communication capability based on MPI library became available only in 2018. Without this, GPU1-GPU2 communication needs to go through two additional CPUs, GPU1 -> CPU1 -> CPU2 -> GPU2, severely compromising the internode communication speed and software scalability. Now, the direct GPU-GPU communication capability at hand, INQ is ready to exploit full capability of CPU+GPU hybrid architecture.

However, in considering future plan, we must understand that the issue is well recognized by the HPC community and industry, and the solutions are coming. The question is when. For example, Apple M1 chip, released in 2020, takes system on chip approach (SOC) where a set of memory together with CPU and GPU are integrated on one chip. HPC system based on such an unique memory architecture can eliminate redundant interconnect as well. Fugaku HPC system in fact takes this approach and demonstrated good overall scalability. A64FX chip developed by Fujitsu for Fugaku is based on ARM architecture, also used by Apple for iPhones and now for Mac mini, with an extension called scalable vector extension (SVE) that emulates vector processing seen in 20th century. ARM recently announced ARMv9 instruction set, which includes a modified SVE called SVE2. In addition, there is ongoing effort by NVIDIA to acquire ARM company based in United Kingdom. Simultaneously, RISC-V offering a similar yet simpler instruction set to ARM has been adopted by MIPS corporation which popularized RISC CPU for high performance workstation in 1990s. Keep these in mind, we anticipate that the next significant change in the architecture of DOE Leadership Class HPC systems is coming during the next few upgrades (5~10 years), and our software development strategy must take this into consideration. 

 The anticipated rapid change of the HPC system architecture, combined with expected growth in number of contributors who are not expert programmers, made us the following choice. While our choices of software development platform github/gitlab offer an elaborate functionality for developing and testing software, the workflow for a specific software needs to be developed by the developers. Accordingly, we have been and will be developing a well-defined workflow for code modification and automated tests that are performed over available HPC systems (currently for Lassen/Sierra and Cori and will be expanded for all the DOE Leadership Class HPC systems as our user base expand). We are also developing a document explaining our workflow, which (in addition to the users/programmers guides of INQ software) will be improved based on feedbacks from users and SABs. These will be critical factors for an open source software development project like ours whose value have to be shared among of all the stake holders such as researchers in scientific and HPC communities, HPC industry and the funding agencies. 
 
Role of all hands meetings and TMF projects. TMF projects are used to expand NPNEQ software users and theory and experiment collaborators. Examples:
Our scientific activities, beta-alumina, perovskite quantum dots, (Aaron had paper under review?)

Role of my participation on co-design center proposal led by Sadas. Why this is important? This is the way to reflect the need of Materials Sciences for the future HPC systems.


This may explain about the current status of GPU ready DFT software around the world. 

Flexibility in design change 

Besides INQ development, the software development team, Andrade, Correa, Pemmaraju have been being engaged in variety of activities that are very important for successful development and distribution of our software,  (interaction with HPC community with a few examples, dissemination through tutorial etc)

HPC simulations performed in scientific studies show range of arithmetic intensities depending on software and problems. This can be studied by profiling the simulations ran at a HPC center. A study conducted for development of HPC systems in Japan indicated that 

Probably results and future plan should introduce the specs of current and upcoming HPC systems

Our intention Discovery of novel phenomena , which may lead to  technological breakthrough.  Analogous story we witnessed at the turn of century, discovery of giant magneto resistance that led to miniaturization of hard desk, which enabled revolutionary products, such as Apple iPod. 

\textcolor{red}{below should be in the next section perhaps}

Understanding the concept of arithmetic intensity is crucial in designing HPC software and hardware. Software with a given algorithm will show a certain ratio of number of floating point operation per machine cycle/number of data copy between memory and register per machine cycle/number of data copy between (MPI) processes per machine cycle, though it may vary slightly depending on some indirect factors. By comparing these ratios between a given simulation against that of hardware (floating point operation count, memory bandwidth, interconnect bandwidth), one can understand approximate upper limit of software performance. In order to understand the performance behavior of parallel GPU (TD)DFT software on a few target HPC systems, Ogitsu is currently in communication with Prof. Stanimire Tomov of U. Tennesee, whose team developed GPU ready parallel 3D FFT library named heFFTe, which is optimized for Summit at ORNL. As it is known, computationally expensive parts of a GGA/LDA level DFT simulation based on planewave pseudopotential algorithm are orthonormalization of wavefunction, 3D-FFT, and non-local pseudopotential. Accordingly, the performance of 3D FFT often becomes a good predictor of overall performance.




\clearpage